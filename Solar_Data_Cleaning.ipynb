{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": { "id": "imports" },
      "outputs": [],
      "source": [
        "import pandas as pd",
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": { "id": "load-data" },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"ng_solar_dataset_10000 - Copy.xlsx\")"
      ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "explore-md" },
        "source": [
            "# 1. Initial Data Exploration"
        ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": { "id": "explore-head" },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": { "id": "explore-info" },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": { "id": "explore-describe" },
      "outputs": [],
      "source": [
        "df.describe(include='all')"
      ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "cleaning-md" },
        "source": [
            "# 2. Data Cleaning"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "missing-md" },
        "source": [
            "## 2.1 Handling Missing Values"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "missing-desc-1" },
        "source": [
            "First, let's check which columns have missing values and how many."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "missing-check" },
        "outputs": [],
        "source": [
            "df.isnull().sum()"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "missing-desc-2" },
        "source": [
            "There are two main strategies for dealing with missing values:",
            "1. **Dropping:** Removing the rows or columns with missing values. This is simple but can lead to significant data loss.",
            "2. **Imputation:** Filling in the missing values with a calculated value (e.g., mean, median, mode). This preserves the data but adds artificial values.",
            "\\n",
            "We will proceed with imputation, as it avoids data loss. We will impute numerical columns with the median (robust to outliers) and categorical columns with the mode (most frequent value)."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "missing-impute" },
        "outputs": [],
        "source": [
            "for col in df.columns:",
            "    if df[col].dtype == 'object':",
            "        # Impute categorical columns with mode",
            "        df[col].fillna(df[col].mode()[0], inplace=True)",
            "    else:",
            "        # Impute numerical columns with median",
            "        df[col].fillna(df[col].median(), inplace=True)"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "missing-desc-4" },
        "source": [
            "Now, let's verify that there are no more missing values."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "missing-verify" },
        "outputs": [],
        "source": [
            "print(f\"Total missing values after imputation: {df.isnull().sum().sum()}\")"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "duplicates-md" },
        "source": [
            "## 2.2 Handling Duplicate Entries"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "duplicates-desc-1" },
        "source": [
            "First, let's check for any duplicate rows in the dataset."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "duplicates-check" },
        "outputs": [],
        "source": [
            "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "duplicates-desc-2" },
        "source": [
            "Now, let's remove them and reset the index."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "duplicates-drop" },
        "outputs": [],
        "source": [
            "df.drop_duplicates(inplace=True)",
            "df.reset_index(drop=True, inplace=True)"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "duplicates-desc-3" },
        "source": [
            "Finally, verify that no duplicates remain."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "duplicates-verify" },
        "outputs": [],
        "source": [
            "print(f\"Number of duplicate rows after cleaning: {df.duplicated().sum()}\")"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "text-md" },
        "source": [
            "## 2.3 Cleaning Text Data (Inconsistencies & Typos)"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "text-desc-1" },
        "source": [
            "Text data can be messy. Common issues include inconsistent capitalization and extra whitespace. We can clean this up by converting all text to lowercase and stripping whitespace from the beginning and end of strings."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "text-get-cols" },
        "outputs": [],
        "source": [
            "text_cols = df.select_dtypes(include=['object']).columns",
            "print(f\"Text columns identified: {list(text_cols)}\")"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "text-desc-2" },
        "source": [
            "Let's look at a sample of the unique values in the first text column before cleaning. Pay attention to any mixed case or extra spaces."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "text-before" },
        "outputs": [],
        "source": [
            "if len(text_cols) > 0:",
            "    print(f\"Unique values in '{text_cols[0]}' before cleaning (sample):\")",
            "    print(df[text_cols[0]].unique()[:5])"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "text-desc-3" },
        "source": [
            "Now, we apply the cleaning transformations to all text columns."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "text-clean" },
        "outputs": [],
        "source": [
            "for col in text_cols:",
            "    df[col] = df[col].str.lower()",
            "    df[col] = df[col].str.strip()"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "text-desc-4" },
        "source": [
            "Let's look at the same column again after cleaning to see the effect. All text should now be lowercase and free of leading/trailing whitespace."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "text-after" },
        "outputs": [],
        "source": [
            "if len(text_cols) > 0:",
            "    print(f\"Unique values in '{text_cols[0]}' after cleaning (sample):\")",
            "    print(df[text_cols[0]].unique()[:5])"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "numerical-md" },
        "source": [
            "## 2.4 Cleaning Numerical Data (Out-of-Range & Outliers)"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "numerical-desc-1" },
        "source": [
            "### 2.4.1 Handling Incorrect or Out-of-Range Values",
            "Some data may contain values that are not logically possible. For example, in a solar dataset, we wouldn't expect to see negative values for measurements like solar radiation. We will check for and correct such values."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "numerical-get-cols" },
        "outputs": [],
        "source": [
            "numeric_cols = df.select_dtypes(include=np.number).columns",
            "print(f\"Checking for negative values in numerical columns: {list(numeric_cols)}\")",
            "for col in numeric_cols:",
            "    negative_count = (df[col] < 0).sum()",
            "    print(f\"- Found {negative_count} negative values in '{col}'.\")"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "numerical-desc-2" },
        "source": [
            "A common strategy for such values is to cap them at a valid minimum (like 0). This is called 'clipping'."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "numerical-clip" },
        "outputs": [],
        "source": [
            "for col in numeric_cols:",
            "    df[col] = df[col].clip(lower=0)",
            "print(\"Clipped all negative values in numerical columns to 0.\")"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "numerical-desc-3" },
        "source": [
            "### 2.4.2 Handling Outliers",
            "Outliers are data points that are significantly different from other observations. They can skew statistical analysis. We will use the Interquartile Range (IQR) method to detect them.",
            "An outlier is a data point that falls outside of `1.5 * IQR` below the first quartile (Q1) or above the third quartile (Q3)."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "numerical-outliers-check" },
        "outputs": [],
        "source": [
            "print(\"Identifying outliers using the IQR method:\")",
            "for col in numeric_cols:",
            "    Q1 = df[col].quantile(0.25)",
            "    Q3 = df[col].quantile(0.75)",
            "    IQR = Q3 - Q1",
            "    lower_bound = Q1 - 1.5 * IQR",
            "    upper_bound = Q3 + 1.5 * IQR",
            "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]",
            "    print(f\"- Found {len(outliers)} outliers in '{col}'.\")"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "numerical-desc-4" },
        "source": [
            "Instead of removing outliers, which can cause data loss, we can cap them. This means any value below the lower bound is set to the lower bound, and any value above the upper bound is set to the upper bound. This is also known as 'winsorizing'."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "numerical-outliers-cap" },
        "outputs": [],
        "source": [
            "for col in numeric_cols:",
            "    Q1 = df[col].quantile(0.25)",
            "    Q3 = df[col].quantile(0.75)",
            "    IQR = Q3 - Q1",
            "    lower_bound = Q1 - 1.5 * IQR",
            "    upper_bound = Q3 + 1.5 * IQR",
            "    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)",
            "print(\"Capped all outliers in numerical columns.\")"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "numerical-desc-5" },
        "source": [
            "Let's verify that the outliers have been handled."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "numerical-outliers-verify" },
        "outputs": [],
        "source": [
            "print(\"Re-checking for outliers after capping:\")",
            "for col in numeric_cols:",
            "    Q1 = df[col].quantile(0.25)",
            "    Q3 = df[col].quantile(0.75)",
            "    IQR = Q3 - Q1",
            "    lower_bound = Q1 - 1.5 * IQR",
            "    upper_bound = Q3 + 1.5 * IQR",
            "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]",
            "    print(f\"- Found {len(outliers)} outliers in '{col}'.\")"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "types-md" },
        "source": [
            "## 2.5 Correcting Data Types"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "types-desc-1" },
        "source": [
            "Ensuring each column has the correct data type is crucial for analysis and memory efficiency. For example, a column of numbers stored as text (object type) can't be used in mathematical calculations."
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "types-desc-2" },
        "source": [
            "Let's look at the current data types."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "types-before" },
        "outputs": [],
        "source": [
            "df.info()"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "types-desc-3" },
        "source": [
            "We can use `pd.to_numeric()` to convert columns that should be numerical and `pd.to_datetime()` for date/time columns. Below is a demonstration.",
            "We use `errors='coerce'`, which will turn any value that cannot be converted into `NaN` (Not a Number). We then have to re-impute these new missing values."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "types-convert" },
        "outputs": [],
        "source": [
            "for col in df.select_dtypes(include=['object']).columns:",
            "    # Attempt to convert object columns to numeric, coercing errors",
            "    # This is a general approach; in a real scenario, you'd target specific columns.",
            "    df[col] = pd.to_numeric(df[col], errors='coerce')",
            "    ",
            "    # Re-impute any NaNs created by the coercion",
            "    if df[col].isnull().sum() > 0:",
            "        df[col].fillna(df[col].median(), inplace=True)",
            "",
            "# Example for converting a date column (replace 'your_date_column' with a real one if it exists)",
            "# if 'your_date_column' in df.columns:",
            "#     df['your_date_column'] = pd.to_datetime(df['your_date_column'], errors='coerce')",
            "#     # For datetime, we might fill with the mode or a specific placeholder",
            "#     if df['your_date_column'].isnull().sum() > 0:",
            "#         df['your_date_column'].fillna(df['your_date_column'].mode()[0], inplace=True)"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "types-desc-4" },
        "source": [
            "Let's check the data types again to confirm the changes."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "types-after" },
        "outputs": [],
        "source": [
            "df.info()"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "merging-md" },
        "source": [
            "## 2.6 Handling Data Merging Issues"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "merging-desc-1" },
        "source": [
            "When working on a real-world project, you often need to combine data from multiple sources. This is done by 'merging' or 'joining' datasets. Pandas provides the powerful `pd.merge()` function for this.",
            "\\n",
            "Key concepts for merging:",
            "- **Keys:** The column(s) that the DataFrames have in common and are used to match rows. For example, a `user_id` column.",
            "- **Join Type:** How to handle rows that don't have a matching key in the other DataFrame.",
            "    - `inner` (default): Keep only the rows where the key exists in **both** DataFrames.",
            "    - `outer`: Keep **all** rows from both DataFrames, filling in `NaN` where there is no match.",
            "    - `left`: Keep all rows from the **left** DataFrame, and only the matching rows from the right.",
            "    - `right`: Keep all rows from the **right** DataFrame, and only the matching rows from the left."
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "merging-desc-2" },
        "source": [
            "Since we only have one dataset here, we can't perform a real merge. However, the code cell below provides a template for how you would do it."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "merging-code" },
        "outputs": [],
        "source": [
            "# Assume you have a second DataFrame called 'df2' that you've loaded",
            "# and it shares a common column, e.g., 'location_id', with our main 'df'.",
            "",
            "# Example DataFrames (uncomment to run)",
            "# data1 = {'location_id': [1, 2, 3], 'temperature': [35, 32, 34]}",
            "# df1 = pd.DataFrame(data1)",
            "# print(\"DataFrame 1:\")",
            "# print(df1)",
            "# ",
            "# data2 = {'location_id': [1, 2, 4], 'manager': ['Ali', 'Binta', 'Charles']}",
            "# df2 = pd.DataFrame(data2)",
            "# print(\"\\nDataFrame 2:\")",
            "# print(df2)",
            "",
            "# --- INNER JOIN ---",
            "# merged_inner = pd.merge(df1, df2, on='location_id', how='inner')",
            "# print(\"\\nInner Join Result:\")",
            "# print(merged_inner) # Will only have location_id 1 and 2",
            "",
            "# --- LEFT JOIN ---",
            "# merged_left = pd.merge(df1, df2, on='location_id', how='left')",
            "# print(\"\\nLeft Join Result:\")",
            "# print(merged_left) # Will have all of df1's rows"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "verify-md" },
        "source": [
            "# 3. Final Verification"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "verify-desc-1" },
        "source": [
            "After all the cleaning steps, let's perform a final check to see the state of our data."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "verify-summary" },
        "outputs": [],
        "source": [
            "print(\"--- Final Data Quality Report ---\")",
            "print(f\"Total Missing Values: {df.isnull().sum().sum()}\")",
            "print(f\"Total Duplicate Rows: {df.duplicated().sum()}\")",
            "print(f\"Final DataFrame Shape: {df.shape}\")",
            "print(\"---------------------------------\")"
        ]
    },
    {
        "cell_type": "markdown",
        "metadata": { "id": "verify-desc-2" },
        "source": [
            "Let's also look at the first few rows of the fully cleaned DataFrame."
        ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": { "id": "verify-head" },
        "outputs": [],
        "source": [
            "df.head()"
        ]
    }
  ]
}